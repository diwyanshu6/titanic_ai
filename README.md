# ğŸš¢ Titanic AI â€” Intelligent Hybrid Data Assistant (Production-Ready)

Titanic AI is a hybrid analytics system that intelligently answers questions about the Titanic dataset using a safe, production-ready routing architecture.

It combines:

- âš¡ Deterministic rule-based analytics (zero hallucination)
- ğŸ“Š Dynamic data visualizations
- ğŸ§  LLM-powered reasoning (Groq + LangChain)
- ğŸ— Modular backend architecture
- ğŸ“¦ Structured production logging
- ğŸš€ Cloud deployment (Render + Streamlit Cloud)

This project demonstrates how to safely integrate LLMs into real-world backend systems without sacrificing reliability, performance, cost efficiency, or performance speed.

---

# ğŸŒ Live Deployment

## Backend (FastAPI on Render)
Production API serving `/chat` endpoint.

## Frontend (Streamlit Cloud)
Interactive UI connected to deployed backend.

The system is fully production-accessible.

---

# ğŸ§  Problem This Project Solves

LLMs are powerful â€” but:

- They hallucinate
- They are expensive
- They are slower than deterministic logic
- They can hit rate limits

## âœ… Solution: 3-Layer Intelligent Routing Architecture

The system routes queries in this order:

Deterministic Engine â†’ Visualization Engine â†’ LLM Engine

LLM is used only when absolutely necessary.

This ensures:

- Maximum reliability
- Minimum hallucination risk
- Low API cost
- Fast response time
- Production safety

---

# ğŸ— System Architecture

User (Streamlit UI)  
        â”‚  
        â–¼  
FastAPI Backend (/chat)  
        â”‚  
        â–¼  
TitanicAgentService (Router)  
        â”‚  
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
 â”‚               â”‚               â”‚  
 â–¼               â–¼               â–¼  
Deterministic   Visualization    LLM Engine  
Engine          Engine           (Groq + LangChain)  

---

# ğŸ”„ Routing Strategy

## ğŸŸ¢ Layer 1 â€” Deterministic Engine (Priority 1)

Handles:

- Counts  
- Multi-filter queries  
- Percentages  
- Grouped counts  
- Survival rate  
- Mean / Max / Min  
- Gender filtering  
- Class filtering  
- Survival filtering  

Why deterministic first?

- No hallucination  
- Instant response  
- Zero API usage  
- Fully production-safe  

If matched â†’ returns immediately.  
LLM is never called.

---

## ğŸ”µ Layer 2 â€” Visualization Engine

Triggered only when query contains:

plot, chart, histogram, scatter, line, boxplot, pie, bar

Generates:

- Matplotlib chart  
- Base64-encoded image  
- Clean structured JSON response  

No LLM involved. Fully deterministic.

---

## ğŸŸ£ Layer 3 â€” LLM Engine (Groq + LangChain)

Used only when:

- Deterministic logic cannot answer  
- Query requires reasoning  
- Correlation or interpretation needed  

Features:

- Pandas DataFrame Agent  
- max_iterations limit (prevents infinite loops)  
- Timeout handling  
- Rate-limit (429) handling  
- Exception protection  
- Structured logging  

---

# ğŸ“ Project Structure
titanic_ai/
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ main.py
â”‚ â”‚
â”‚ â”œâ”€â”€ core/
â”‚ â”‚ â”œâ”€â”€ config.py
â”‚ â”‚ â”œâ”€â”€ exceptions.py
â”‚ â”‚ â”œâ”€â”€ exceptions_handler.py
â”‚ â”‚ â””â”€â”€ logging_config.py
â”‚ â”‚
â”‚ â”œâ”€â”€ services/
â”‚ â”‚ â”œâ”€â”€ agent_service.py
â”‚ â”‚ â”œâ”€â”€ deterministic_engine.py
â”‚ â”‚ â”œâ”€â”€ visualization_engine.py
â”‚ â”‚ â””â”€â”€ llm_engine.py
â”‚ â”‚
â”‚ â”œâ”€â”€ schemas/
â”‚ â”‚ â””â”€â”€ chat.py
â”‚ â”‚
â”‚ â””â”€â”€ data/
â”‚ â””â”€â”€ titanic.csv
â”‚
â”œâ”€â”€ frontend/
â”‚ â””â”€â”€ app.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

# ğŸ§© Core Components Explained

## 1ï¸âƒ£ DeterministicEngine

Rule-based analytics engine.

Supports:

- Gender filtering  
- Class filtering  
- Survival filtering  
- Multi-filter queries  
- Grouped counts  
- Percentages  
- Survival rate analysis  
- Numeric operations (mean, max, min)  

Key Design Decisions:

- Grouped logic evaluated before total count  
- Prevents wrong early returns  
- Avoids unnecessary LLM calls  
- Ensures deterministic reliability  

---

## 2ï¸âƒ£ VisualizationEngine

Handles explicit visual requests.

Supported plots:

- Histogram  
- Scatter plot  
- Line plot  
- Boxplot  
- Pie chart  
- Bar chart  

Returns:

- Base64 encoded image  
- Structured JSON response  

Fully deterministic, no hallucination risk.

---

## 3ï¸âƒ£ LLMEngine

Powered by:

- Groq API  
- LangChain Pandas Agent  

Safety Mechanisms:

- max_iterations limit  
- Rate limit handling (429)  
- Timeout handling  
- Parsing error protection  
- Fallback safety  

Used only when deterministic fails.

---

# ğŸ“Š Logging System

Structured JSON logging implemented using a custom formatter.

Logs include:

- request_id  
- query  
- latency  
- visualization flag  
- token usage  
- hallucination detection flag  

Example:

Rotating file handler included.

---

# ğŸ” Safety & Reliability Features

- Deterministic-first routing  
- LLM iteration limit  
- Timeout protection  
- Rate-limit retry handling  
- Structured exception handling  
- Invalid query detection  
- JSON response standardization  

---

# ğŸš€ Deployment

## Backend (Render)

Start Command:
uvicorn backend.main:app --host 0.0.0.0 --port 10000

Environment Variables:

- GROQ_API_KEY  
- MODEL_NAME  

---

## Frontend (Streamlit Cloud)

Frontend connects to deployed backend:
API_URL = "https://titanic-backend-klbp.onrender.com/chat"

---

# ğŸ§ª Example Queries

## Deterministic Queries

- How many third class passengers survived?  
- What percentage of passengers were male?  
- How many passengers embarked from each port?  
- What was the average Fare?  
- Which class had the highest survival rate?  

## Visualization Queries

- Show histogram of Age  
- Scatter plot of Fare vs Age  
- Pie chart of Embarked  
- Bar chart of Pclass  

## LLM Queries

- Were women treated better?  
- Find correlation between fare and survival  
- Compare survival across social classes  

---

# âš™ï¸ Local Setup

## Clone
git clone https://github.com/diwyanshu6/titanic_ai.git

## Install

cd titanic_ai
pip install -r requirements.txt

## Run Backend


uvicorn backend.main:app --reload

## Run Frontend
streamlit run frontend/app.py


---

# ğŸ¯ Design Philosophy

This project demonstrates:

- Hybrid AI architecture  
- Deterministic-first system design  
- Production-safe LLM integration  
- Modular backend structure  
- Logging-driven debugging  
- Cloud deployment workflow  
- Error-resilient API design  

---

# ğŸ“ˆ Future Improvements

- Query caching  
- Automated testing suite  
- Swagger documentation enhancement  
- Query history storage  
- Advanced NLP preprocessing  
- Dockerization  

---

# ğŸ§  What This Project Demonstrates

âœ” Backend system design  
âœ” AI routing architecture  
âœ” Hallucination mitigation strategy  
âœ” Cloud deployment workflow  
âœ” Production logging  
âœ” LLM safety management  
âœ” Real-world debugging  

---

# ğŸ‘¤ Author

Diwyanshu  
AI Systems & Backend Engineering  

---


